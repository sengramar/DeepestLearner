# -*- coding: utf-8 -*-
"""inception.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p_dsRhfr1TpOWivDKqImFJ7XtAQ5w1NF

# SETUP
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

import keras
from keras.layers.core import Layer
import keras.backend as K
import tensorflow as tf
import os

from keras.models import Model
from keras.layers import Conv2D, MaxPool2D,  \
    Dropout, Dense, Input, concatenate,      \
    GlobalAveragePooling2D, AveragePooling2D,\
    Flatten

import cv2 
import numpy as np 
from keras.datasets import cifar10 
from keras import backend as K 
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img

import math 
from keras.optimizers import SGD 
from keras.callbacks import LearningRateScheduler

"""# INCEPTION PARTS"""

def inception_module(x,
                     filters_1x1,
                     filters_3x3_reduce,
                     filters_3x3,
                     filters_5x5_reduce,
                     filters_5x5,
                     filters_pool_proj,
                     name=None):
    
    # 1X1 CONV
    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)
    
    # 1X1 CONV --> 3x3 CONV
    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)
    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)

    # 1X1 CONV --> 5x5 CONV
    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)
    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)

    # 3X3 MAXPOOL --> 1X1 CONV
    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)
    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)

    # Concatenate the layers
    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)
    
    return output

kernel_init = keras.initializers.glorot_uniform()
bias_init = keras.initializers.Constant(value=0.2)

"""# MODEL"""

input_layer = Input(shape=(32,32,3))

# Add the first CONV layer and MaxPOOL layer
x = Conv2D(64, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_1_3x3/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)
x = MaxPool2D((3, 3), padding='same', strides=(1, 1), name='max_pool_1_3x3/2')(x)

## Add Inception layer-1
x = inception_module(x,
                     filters_1x1=64,
                     filters_3x3_reduce=96,
                     filters_3x3=128,
                     filters_5x5_reduce=16,
                     filters_5x5=32,
                     filters_pool_proj=32,
                     name='inception_3a')

## Add Inception layer-2
x = inception_module(x,
                     filters_1x1=128,
                     filters_3x3_reduce=128,
                     filters_3x3=192,
                     filters_5x5_reduce=32,
                     filters_5x5=96,
                     filters_pool_proj=64,
                     name='inception_3b')

## Add MaxPool
x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool2_3x3/2')(x)

## Add Inception layer-3
x = inception_module(x,
                     filters_1x1=192,
                     filters_3x3_reduce=96,
                     filters_3x3=208,
                     filters_5x5_reduce=16,
                     filters_5x5=48,
                     filters_pool_proj=64,
                     name='inception_3c') 

## Add Inception layer-4
x = inception_module(x,
                     filters_1x1=160,
                     filters_3x3_reduce=112,
                     filters_3x3=224,
                     filters_5x5_reduce=24,
                     filters_5x5=64,
                     filters_pool_proj=64,
                     name='inception_3d') 

## Add Avg Pool
x = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)

## Add Dropout
x = Dropout(0.4)(x)

## Add Dense layer
x = Dense(6, activation='softmax', name='output')(x)

model = Model(input_layer, x, name='inception_v1')
model.summary()

"""# Dataset"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive/deep/assignment_two
!ls
!pwd

data_dir = '/content/gdrive/MyDrive/deep/assignment_two/Garbage classification/Garbage classification'
classes = os.listdir(data_dir)
print(f"classes: {classes}")

train_datagen = ImageDataGenerator(rescale = 1/255., validation_split=0.15)

test_datagen = ImageDataGenerator(rescale=1/255.0, validation_split=0.15)

train_generator = train_datagen.flow_from_directory(data_dir,
                                                    batch_size = 32,
                                                    shuffle=True,
                                                    target_size = (32, 32),
                                                    subset = "training",
                                                    class_mode='categorical')

val_generator = test_datagen.flow_from_directory(data_dir,
                                                  batch_size = 32,
                                                  shuffle=True,
                                                  target_size = (32,32),
                                                  subset = 'validation',
                                                  class_mode='categorical')

test_generator = test_datagen.flow_from_directory(data_dir,
                                                    batch_size = 32,
                                                    shuffle=True,
                                                    target_size = (32, 32),
                                                    subset = 'validation',
                                                    class_mode='categorical')

labels = (train_generator.class_indices)
labels = dict((v, k) for k, v in labels.items())
print(labels)

for d_batch, l_batch in train_generator:
  print(f"Data batch shape: {d_batch.shape}")
  print(f"Label batch shape: {l_batch.shape}")
  break

l_batch

"""# FITTING"""

#Define the number of epochs and learning rate
epochs = 25
initial_lrate = 0.01

#Create Learning rate decay
def decay(epoch, steps=100):
    initial_lrate = 0.01
    drop = 0.96
    epochs_drop = 8
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return lrate

#Define SGD paramenters
sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)

lr_sc = LearningRateScheduler(decay, verbose=1)

## Compile Model
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

history1 = model.fit(train_generator, validation_data=(val_generator), epochs=epochs, callbacks=[lr_sc])

